import numpy as np
import ember
from functions import *
from sklearn import preprocessing

## Load Dataset
X_dump, y_dump, X, y = ember.read_vectorized_features("data/ember2018/")
metadata_frame = ember.read_metadata("data/ember2018/")

## Seperate Training and Testing Data
X_train = X[:100000]
y_train = y[:100000].astype(int)
X_test = X[100000:130000]
y_test = y[100000:130000].astype(int)

## Normalize and Pre-Process Data
X_train = preprocessing.normalize(X_train, axis=1)
X_train = preprocessing.normalize(X_train, axis=0)

X_test = preprocessing.normalize(X_test, axis=1)
X_test = preprocessing.normalize(X_test, axis=0) 

y_test = np.array([y_test])
test_set_x = X_test.T

def train(epochs, layers, lr, batch_size):
    data_size = X_train.shape[0]
    n_batches = data_size/batch_size

    ## Initialize Model
    layer_dims = [X_train.shape[1], layers, 1]
    parameters = initialize_parameters(layer_dims)

    ## Train Model
    b = 0
    costs = []
    num_of_layers = len(layer_dims) # including input layer
    hidden_act_type = 'relu'
    output_act_type = 'sigmoid'
    num_of_iterations = epochs
    while b < n_batches:
        print("Batch:", b)

        ## Transform Data
        train_set_y = np.array([y_train[batch_size*b:batch_size*(b+1)].T])
        train_set_x = X_train[batch_size*b:batch_size*(b+1)].T


        for i in range(num_of_iterations): 
            input_data = train_set_x
            caches = [] # list with each element being a tuple (A_prev,W,Z) of each layer
            grads = {}  # dictionary of dW and db for each layer
            
            ## Forward Prop
            # for hidden layers:     
            for l in range(1,num_of_layers-1):
                cache, A = forward_layer(input_data, parameters['W'+str(l)], parameters['b'+str(l)], hidden_act_type)
                caches.append(cache)   # append tuple to list
                input_data = A 
            # for output layer:     
            cache, y_hat = forward_layer(input_data, parameters['W'+str(num_of_layers-1)], parameters['b'+str(num_of_layers-1)], output_act_type)
            caches.append(cache) # caches will have indeces [0, .. ,num_of_layers-2]

                        
            ## Compute Cost (for entire train set examples)
            cost = compute_cost(train_set_y, y_hat, loss_type='binary_cross_entropy') 
            

            ## Backward Prop
            # for output layer:
            AL = y_hat
            Y = train_set_y.reshape(AL.shape)
            dA = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # dA of output layer,
                                                #dependent on activation and loss type
            A_prev, W, Z = caches[num_of_layers-2] 
            dA_prev, dW, db = backward_layer(dA, Z, A_prev, W, output_act_type) 
            grads['dW' + str(num_of_layers - 1)] = dW
            grads['db' + str(num_of_layers - 1)] = db
            # for hidden layers:
            for l in reversed(range(1,num_of_layers-1)):    # starts at l = num-2  
                dA = dA_prev
                A_prev, W, Z = caches[l-1] 
                dA_prev, dW, db = backward_layer(dA, Z, A_prev, W, hidden_act_type)
                grads['dW' + str(l)] = dW
                grads['db' + str(l)] = db


            ## Update Weight and Bias
            for l in range(1,num_of_layers):
                optimize(parameters['W'+str(l)], grads['dW'+str(l)], lr)
                optimize(parameters['b'+str(l)], grads['db'+str(l)], lr)

            
            # Print Cost
            if i % (num_of_iterations//10) == 0:
                costs.append(cost)
                print("Cost after iteration {}: {}".format(i,np.squeeze(cost)))
            
        ## Increment Batch
        b = b + 1

    ## Test Model
    predict(parameters, test_set_x, y_test, 'relu','sigmoid')
## Run Model
train(100, 64, 0.025, 500)
